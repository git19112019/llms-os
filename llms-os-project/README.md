# LLMs OS Docker Project - Enhanced Edition

ğŸš€ Production-ready workflow automation system with integrated Mock API, monitoring, and testing capabilities.

## Features

- âœ… **Docker-based deployment** - Multi-stage builds for minimal image sizes
- ğŸ”Œ **Mock OpenRouter API** - Test without real API keys
- ğŸ“Š **Built-in monitoring** - Prometheus + Grafana integration
- âš¡ **Async execution** - Parallel task processing
- ğŸ” **Plugin system** - Extensible architecture
- ğŸ§ª **Comprehensive testing** - Unit and integration tests
- ğŸ”’ **Security hardened** - Non-root containers, health checks

## Quick Start

### 1. Setup Environment

```bash
# Copy environment template
cp .env.example .env

# Build Docker images
make build

# Start all services
make up
```

### 2. Run Test Workflow

```bash
# Run basic test
docker-compose run --rm llms-os workflows/test_basic.yaml

# Run advanced test with parallel execution
docker-compose run --rm llms-os workflows/test_advanced.yaml
```

### 3. Access Services

- **Mock API**: http://localhost:8000
- **API Health**: http://localhost:8000/health
- **API Metrics**: http://localhost:8000/metrics

## Project Structure

```
llms-os-project/
â”œâ”€â”€ docker-compose.yml              # Main services
â”œâ”€â”€ docker-compose.dev.yml          # Development environment
â”œâ”€â”€ docker-compose.monitoring.yml   # Monitoring stack
â”œâ”€â”€ Makefile                        # Build automation
â”œâ”€â”€ .env.example                    # Environment template
â”‚
â”œâ”€â”€ llms-os/                        # Main application
â”‚   â”œâ”€â”€ Dockerfile                  # Production image
â”‚   â”œâ”€â”€ Dockerfile.dev              # Development image
â”‚   â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ LLMs_OS/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ registry.py         # Action registry
â”‚       â”‚   â”œâ”€â”€ core.py             # Sync execution
â”‚       â”‚   â”œâ”€â”€ async_core.py       # Async execution
â”‚       â”‚   â”œâ”€â”€ cli.py              # CLI interface
â”‚       â”‚   â”œâ”€â”€ exceptions.py       # Custom exceptions
â”‚       â”‚   â”œâ”€â”€ validators.py       # Input validation
â”‚       â”‚   â”œâ”€â”€ monitoring.py       # Metrics collection
â”‚       â”‚   â”œâ”€â”€ plugins.py          # Plugin system
â”‚       â”‚   â””â”€â”€ actions/
â”‚       â”‚       â”œâ”€â”€ print_message.py
â”‚       â”‚       â”œâ”€â”€ chat_completion.py
â”‚       â”‚       â”œâ”€â”€ http_request.py
â”‚       â”‚       â””â”€â”€ file_operations.py
â”‚       â””â”€â”€ tests/                  # Test suite
â”‚
â”œâ”€â”€ mock-api/                       # Mock OpenRouter API
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ monitoring/                     # Monitoring configs
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚
â””â”€â”€ workflows/                      # Example workflows
    â”œâ”€â”€ test_basic.yaml
    â””â”€â”€ test_advanced.yaml
```

## Available Make Commands

```bash
make help         # Show all commands
make build        # Build Docker images
make up           # Start all services
make down         # Stop all services
make dev          # Start development environment
make test         # Run tests
make monitoring   # Start monitoring stack (Prometheus + Grafana)
make logs         # Show logs
make shell        # Open shell in container
make clean        # Clean up everything
```

## Available Actions

The following actions are built-in and ready to use:

- **print_message** - Display formatted messages
- **chat_completion** - Call LLM APIs (OpenRouter compatible)
- **http_request** - Make HTTP requests
- **file_read** - Read file content
- **file_write** - Write content to files

## Example Workflow

```yaml
metadata:
  title: "My Workflow"
  version: "1.0.0"

tasks:
  - action: print_message
    message: "ğŸš€ Starting workflow..."
    style: info
  
  - action: http_request
    url: "http://mock-api:8000/health"
    method: GET
    save_as: health_check
  
  - action: chat_completion
    model: "openai/gpt-3.5-turbo"
    messages:
      - role: user
        content: "Write a haiku about Docker"
    save_as: haiku
  
  - action: print_message
    message: "AI Response: {{ haiku.content }}"
    style: success
```

## Monitoring

Start the monitoring stack:

```bash
make monitoring
```

Access dashboards:
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)

## Development Mode

Start development environment with hot-reload:

```bash
make dev
```

Code changes are automatically reloaded without rebuilding images.

## Testing

Run the test suite:

```bash
make test
```

## Configuration

Environment variables (`.env` file):

```bash
# API Configuration
OPENROUTER_API_URL=http://mock-api:8000/api/v1
OPENROUTER_API_KEY=sk-simulated-key
MOCK_API_KEY=sk-simulated-key

# Logging
LOG_LEVEL=INFO

# Monitoring
ENABLE_METRICS=true
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
```

## Docker Image Sizes

- **llms-os**: ~168MB (Alpine-based, multi-stage build)
- **mock-api**: ~146MB (Slim-based)

## Requirements

- Docker 20.10+
- Docker Compose 2.0+
- 2GB RAM minimum
- 5GB disk space

## License

MIT License - see LICENSE file for details

## Support

For issues and questions, please open an issue on the GitHub repository.

---

**Built with â¤ï¸ using Python, Docker, and LLMs**
